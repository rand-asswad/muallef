<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Background | Automatic Music Transcription</title>
  <meta name="description" content="1 Background | Automatic Music Transcription" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Background | Automatic Music Transcription" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="img/cover_img.png" />
  
  <meta name="github-repo" content="rand-asswad/muallef" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Background | Automatic Music Transcription" />
  
  
  <meta name="twitter:image" content="img/cover_img.png" />

<meta name="author" content="Rand ASSWAD" />


<meta name="date" content="2020-03-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="0_intro.html"/>
<link rel="next" href="2_pitch.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="include/gitbook.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="https://www.insa-rouen.fr/" target="_blank"><img src="img/logo_insa.png"></a></li>
<li class="toc-title"><a href="./">Automatic Music Transcription</a></li>
<li class="toc-author"><a href="/" target="_blank">Rand ASSWAD</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="0_intro.html"><a href="0_intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="1_background.html"><a href="1_background.html"><i class="fa fa-check"></i><b>1</b> Background</a><ul>
<li class="chapter" data-level="1.1" data-path="1_background.html"><a href="1_background.html#automatic-music-transcription"><i class="fa fa-check"></i><b>1.1</b> Automatic Music Transcription</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1_background.html"><a href="1_background.html#history-and-community"><i class="fa fa-check"></i><b>1.1.1</b> History and community</a></li>
<li class="chapter" data-level="1.1.2" data-path="1_background.html"><a href="1_background.html#motivation"><i class="fa fa-check"></i><b>1.1.2</b> Motivation</a></li>
<li class="chapter" data-level="1.1.3" data-path="1_background.html"><a href="1_background.html#underlying-tasks"><i class="fa fa-check"></i><b>1.1.3</b> Underlying tasks</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1_background.html"><a href="1_background.html#physical-definition-of-acoustic-waves"><i class="fa fa-check"></i><b>1.2</b> Physical definition of acoustic waves</a></li>
<li class="chapter" data-level="1.3" data-path="1_background.html"><a href="1_background.html#perception-of-sound-and-music"><i class="fa fa-check"></i><b>1.3</b> Perception of sound and music</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1_background.html"><a href="1_background.html#fundamental-frequency-and-pitch"><i class="fa fa-check"></i><b>1.3.1</b> Fundamental frequency and pitch</a></li>
<li class="chapter" data-level="1.3.2" data-path="1_background.html"><a href="1_background.html#perception-of-intensity"><i class="fa fa-check"></i><b>1.3.2</b> Perception of intensity</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1_background.html"><a href="1_background.html#audio-signal-processing"><i class="fa fa-check"></i><b>1.4</b> Audio signal processing</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1_background.html"><a href="1_background.html#discrete-time-signals"><i class="fa fa-check"></i><b>1.4.1</b> Discrete-time signals</a></li>
<li class="chapter" data-level="1.4.2" data-path="1_background.html"><a href="1_background.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>1.4.2</b> Discrete Fourier Transform (DFT)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2_pitch.html"><a href="2_pitch.html"><i class="fa fa-check"></i><b>2</b> Pitch analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="2_pitch.html"><a href="2_pitch.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="2_pitch.html"><a href="2_pitch.html#single-pitch"><i class="fa fa-check"></i><b>2.2</b> Single pitch</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2_pitch.html"><a href="2_pitch.html#time-domain"><i class="fa fa-check"></i><b>2.2.1</b> Time domain</a></li>
<li class="chapter" data-level="2.2.2" data-path="2_pitch.html"><a href="2_pitch.html#spectral-domain"><i class="fa fa-check"></i><b>2.2.2</b> Spectral domain</a></li>
<li class="chapter" data-level="2.2.3" data-path="2_pitch.html"><a href="2_pitch.html#application-example"><i class="fa fa-check"></i><b>2.2.3</b> Application Example</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2_pitch.html"><a href="2_pitch.html#multiple-pitch"><i class="fa fa-check"></i><b>2.3</b> Multiple pitch</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2_pitch.html"><a href="2_pitch.html#harmonic-amplitudes-sum"><i class="fa fa-check"></i><b>2.3.1</b> Harmonic Amplitudes Sum</a></li>
<li class="chapter" data-level="2.3.2" data-path="2_pitch.html"><a href="2_pitch.html#spectral-factorisation"><i class="fa fa-check"></i><b>2.3.2</b> Spectral factorisation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3_onset.html"><a href="3_onset.html"><i class="fa fa-check"></i><b>3</b> Temporal segmentation</a><ul>
<li class="chapter" data-level="3.1" data-path="3_onset.html"><a href="3_onset.html#introduction-2"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="3_onset.html"><a href="3_onset.html#onset-detection-function-odf"><i class="fa fa-check"></i><b>3.2</b> Onset Detection Function (ODF)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3_onset.html"><a href="3_onset.html#high-frequency-content-hfc"><i class="fa fa-check"></i><b>3.2.1</b> High Frequency Content (HFC)</a></li>
<li class="chapter" data-level="3.2.2" data-path="3_onset.html"><a href="3_onset.html#phase-deviation"><i class="fa fa-check"></i><b>3.2.2</b> Phase Deviation</a></li>
<li class="chapter" data-level="3.2.3" data-path="3_onset.html"><a href="3_onset.html#complex-distance"><i class="fa fa-check"></i><b>3.2.3</b> Complex Distance</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3_onset.html"><a href="3_onset.html#thresholding-peak-picking"><i class="fa fa-check"></i><b>3.3</b> Thresholding &amp; Peak-picking</a></li>
<li class="chapter" data-level="3.4" data-path="3_onset.html"><a href="3_onset.html#results"><i class="fa fa-check"></i><b>3.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4_conclusion.html"><a href="4_conclusion.html"><i class="fa fa-check"></i><b>4</b> Conclusion</a></li>
<li class="chapter" data-level="" data-path="4_conclusion.html"><a href="4_conclusion.html#references"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="book.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF book</a></li>
<li><a href="onepage.html" target="_blank"><i class="fa fa-file-code-o"></i> HTML standalone version</a></li>
<li><a href="presentation.html" target="_blank"><img class="icon" src="include/presentation.svg"> Presentation slides</a></li>
<li><a href="https://github.com/rand-asswad/muallef" target="_blank"><i class="fa fa-github"></i> Github repository</a></li>
<li><a href="https://github.com/rand-asswad/muallef/archive/refs/heads/master.zip"><i class="fa fa-download"></i> Download source code</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Automatic Music Transcription</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="background" class="section level1">
<h1><span class="header-section-number">1</span> Background</h1>
<p>The focus of this project is music information retrieval
from music audio signals.
In this section we go through the main problems in
the discipline of Automatic Music Transcription,
we study characteristics of musical elements,
human perception of music, and basic notions
of modern music theory.
We also review the main characteristics of a sound wave
as well as analytic tools for processing digital audio signals.
Furthermore, we establish the bridge between music
theory and physical properties of audio signals.</p>
<div id="automatic-music-transcription" class="section level2">
<h2><span class="header-section-number">1.1</span> Automatic Music Transcription</h2>
<blockquote>
<p>AMT is the process of converting an acoustic musical
signal into some form of musical notation. <span class="citation">(Benetos et al. <a href="#ref-benetos_2013">2013</a>)</span></p>
</blockquote>
<div id="history-and-community" class="section level3">
<h3><span class="header-section-number">1.1.1</span> History and community</h3>
<p>The interest in the task of AMT has started in the late
20<sup>th</sup> century, with researchers borrowing and adapting
concepts from the well-established domain of
<em>speech-processing</em>.
Major strides have been made in the 21<sup>st</sup> century,
particularly since the creation of the International
Society for Music Information Retrieval <strong>(ISMIR)</strong> in 2000.
Which have connected the community and provided a platform
for sharing and learning Music Information Retrieval <strong>(MIR)</strong>
concepts worldwide. <span class="citation">(Müller <a href="#ref-muller_2015">2015</a>)</span></p>
<p>furthermore, the Music Information Retrieval Evaluation eXchange
<strong>(MIREX)</strong> is an annual evaluation compaign for MIR algorithms.
Since it started in 2005, MIREX has served as a benchmark
for evaluating novelty algorithms and helped advance
MIR Research.</p>
</div>
<div id="motivation" class="section level3">
<h3><span class="header-section-number">1.1.2</span> Motivation</h3>
<p>MIR and AMT can be of great interest for different demographics.
First, most musicians stand to benefit from reliable
transcription algorithms as it can facilitate their tasks in
difficult cases or for the least accelerate the process.</p>
<p>Moreover, in many music genres such as Jazz, musical
notation is rarely used, therefore the exchange formats
are almost exclusively recordings of performances.
AMT would play a role in democratizing no-score music
for new learners and provide an easier canonical
format for exchanging music.</p>
<p>Another use of MIR is score-following software development
that include a cursor that follows real-time playing
indicating the correct and incorrect notes played
helping pupils practice and progress on their
own more efficiently, making the task of music learning
less painful.</p>
<p>Furthermore, MIR allows performing musicological analysis
directly on recordings, gaining access to much larger
databases compared to anotated music, which can also
be applied for various tasks such as music recognition
or melody recognition.</p>
</div>
<div id="underlying-tasks" class="section level3">
<h3><span class="header-section-number">1.1.3</span> Underlying tasks</h3>
<p>Automatic Music Transcription is divided into several subtasks
where each represents a research topic that fall
within the scope of Musical Information Retrieval.</p>
<p>The largest topic of MIR is tonal analysis, which
is based on analysing spectral features of audio signals,
and subsequently estimating <em>pitch</em>, melody and harmony.
Despite the large interest in this topic and various
techniques applied, this task remains the core problem in AMT,
Exploration of main pitch analysis techniques is the
first part of this project.</p>
<p>Another main AMT task is <em>temporal segmentation</em>,
which relates consequently to rythme extraction and tempo
detection in melodic sounds. <span class="citation">(Benetos et al. <a href="#ref-benetos_2013">2013</a>)</span>
This task pertains pertains to spectral features
as well as signal energy.
We expore this topic in the second part of this project.</p>
<p>Several more tasks are needed to fully transcribe a musical
piece, including: loudness estimation, instrument recognition,
rhythm detection, scale detection and harmony analysis.
In the scope of this project, we limit our study to
<em>pitch analysis</em> and <em>temporal segmentation</em>.</p>
</div>
</div>
<div id="physical-definition-of-acoustic-waves" class="section level2">
<h2><span class="header-section-number">1.2</span> Physical definition of acoustic waves</h2>
<p>Sound is generated by vibrating objects, these vibrations
cause oscillations of molecules in the medium.
The varying pressure propagates through the medium as a wave,
the pressure is therefore the solution of the wave
equation in time and space, also known as the acoustic
wave equation. <span class="citation">(Feynman <a href="#ref-feynman">1965</a>)</span>
<span class="math display">\[\Delta p =\frac{1}{c^2}\frac{\partial^2 p}{ {\partial t}^2}\]</span>
where <span class="math inline">\(p\)</span> is the accoustic pressure function of time and space
and <span class="math inline">\(c\)</span> is the speed of sound propagation.
The wave equation can be solved analytically with the
separation of variables method, resulting in a <em>sinusoidal
harmonic</em> solutions.</p>
<p>In audio signal processing, we are interested in the pressure
at the receptor’s position (listener or microphone),
hence the pressure as a function of time.
An audio signal is therefore defined as the deviation
of pressure from the average pressure of the medium
at the receptor’s position.</p>
<p>The pressure function being harmonic, the sound signal
is of the form
<span class="math display">\[\tilde{x}(t) = \sum_{h=0}^{\infty} A_h \cos(2\pi hf_0t + \varphi_h)\]</span>
where</p>
<ul>
<li><span class="math inline">\(f_0\)</span> is called the <strong>fundamental frequency</strong> of the signal,</li>
<li><span class="math inline">\(h\)</span> is the harmonic number,</li>
<li><span class="math inline">\(A_h\)</span> is the amplitude of the <span class="math inline">\(h^\text{th}\)</span> harmonic,</li>
<li><span class="math inline">\(\varphi_h\)</span> is the phase of the <span class="math inline">\(h^\text{th}\)</span> harmonic.</li>
</ul>
<p>In many works this formula appears in terms of the angular
frequency <span class="math inline">\(\omega=2\pi f\)</span>, we denote as well
<span class="math inline">\(f_h = h f_0\)</span> for <span class="math inline">\(h\geq 1\)</span>.</p>
<p>As harmonics represent proper multiples of the fundamental
frequency, <span class="math inline">\(h=0\)</span> is excluded from the sum
<span class="math display">\[\tilde{x}(t) = a_0 + \sum_{h=1}^{\infty} A_h \cos(2\pi hf_0t + \varphi_h)\]</span>
with <span class="math inline">\(a_0 = A_0\cos(\varphi_0)\)</span>.</p>
</div>
<div id="perception-of-sound-and-music" class="section level2">
<h2><span class="header-section-number">1.3</span> Perception of sound and music</h2>
<p>The human auditory system is capable of distinguishing
intensities and frequencies of sound waves as well as
temporal features.
The inner ear is extremely sensitive to sound wave features,
the brain allows further analysis of these features.</p>
<p>Music theory defines and studies <em>perceived features</em>
of music signals.
These features are based on the signal’s intensity,
frequency, and time patterns.</p>
<p>In music theory, a <strong>note</strong> is a musical symbol that
represents the smallest musical object.
The note’s attributes define the <em>pitch</em> of the sound, its
<em>relative duration</em> and its <em>relative intensity</em>.</p>
<div id="fundamental-frequency-and-pitch" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Fundamental frequency and pitch</h3>
<p>Sound signals are periodic, therefore by definition there
exists a <span class="math inline">\(T&gt;0\)</span> such as
<span class="math display">\[\forall t, \tilde{x}(t)=\tilde{x}(t+T)\]</span>
which follows that there exists an infinite set of values of <span class="math inline">\(T&gt;0\)</span> that verify this property, indeed
<span class="math inline">\(\forall n\in\mathbb{N}, T&#39;=nT, \tilde{x}(t)=\tilde{x}(t+T&#39;)\)</span>.
We define the period of a signal as the smallest positive
value of <span class="math inline">\(T\)</span> for which the property holds.
The <strong>fundamental frequency</strong> <span class="math inline">\(f_0\)</span> is defined formally
as the reciprocal of the period.
This definition holds for <em>any</em> periodic signal,
regardless of its form.</p>
<p>In the case of sound wave, the <em>perception of the fundamental
frequency</em> is referred to as the <strong>pitch</strong>.
Pitch is the defined as the <em>tonal height</em> of a sound,
it is closely related to the fundamental frequency
however remaining a <em>relative musical concept</em>
unlike the <span class="math inline">\(f_0\)</span> of a signal that is an absolute
mathematical value.
In fact, the relation between pitch and <span class="math inline">\(f_0\)</span>
is neither bijective nor invariant.</p>
<p>In music theory, pitch is defined on a discrete space
unlike the continuous frequency space.
Moreover, human perception of frequency is logarithmic
hence obtaining the <em>next pitch</em> corresponds to the
multiplication of the frequency by a certain value <span class="math inline">\(r\)</span>.</p>
<p>Finally, the frequency of the reference pitch A<sub>4</sub>
is widely accepted today as <span class="math inline">\(440 Hz\)</span> while in the baroque
era it was around <span class="math inline">\(415 Hz\)</span> and <span class="math inline">\(440 Hz\)</span> was the frequency
corresponding to A♯ pitch.
Even in modern day, variations of the pitch frequency
exist in different regions and even different orchestras!</p>
</div>
<div id="perception-of-intensity" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Perception of intensity</h3>
<p>Sound intensity is defined physically as the power carried
by sound waves per unit area, whereas sound pressure is
the local pressure deviation from the ambient atmospheric
pressure caused by a sound wave.
Human perception of intensity is directly sensitive to
sound pressure, it is measured in terms of <em>sound pressure level</em> (SPL)
which is a logarithmic measure of sound pressure <span class="math inline">\(P\)</span>
relative to the atmospheric pressure <span class="math inline">\(P_0\)</span> measured
in decibels <span class="math inline">\(\mathrm{dB}\)</span>.
<span class="math display">\[\mathrm{SPL} = 20\log_{10}\left(\frac{P}{P_0}\right) \mathrm{dB}\]</span></p>
<p>Nevertheless, sensitivity to sound intensity is variable
across different frequencies.
The subjective perception of sound pressure
is defined by a sound’s <strong>loudness</strong> which is a function of
both SPL and frequency ranging from quiet to loud.</p>
<p><img src="img/loudness.png" style="width:60.0%" /></p>
<p>In music theory, loudness is defined by a piece’s <strong>dynamics</strong>.
Dynamics are indicators of a part’s loudness <em>relative</em>
to other parts and/or instruments.
Dynamics markings are expressed with the italian
keywords <em>forte</em> <span class="math inline">\(\boldsymbol{f}\)</span> (loud) and <em>piano</em> <span class="math inline">\(\boldsymbol{p}\)</span> (soft).
Subtle degrees of loudness can be expressed
by the prefixes <em>mezzo-</em> or <em>più</em>, for example <span class="math inline">\(\boldsymbol{mp}\)</span> stands
for <em>mezzo-piano</em> (moderately soft) and <span class="math inline">\(pi\grave{u}~\boldsymbol{p}\)</span> (softer),
or by consecutive letters such as <em>fortissimo</em> <span class="math inline">\(\boldsymbol{f}\hspace{-2pt}\boldsymbol{f}\)</span>
(very loud) or more letters if needed.</p>
<p>Music dynamics also allow expressing gradual changes
in loudness, indicated as symbols or italian keywords
(<em>crescendo</em> and <em>diminuendo</em>).</p>
</div>
</div>
<div id="audio-signal-processing" class="section level2">
<h2><span class="header-section-number">1.4</span> Audio signal processing</h2>
<div id="discrete-time-signals" class="section level3">
<h3><span class="header-section-number">1.4.1</span> Discrete-time signals</h3>
<p>The domain of audio signal processing deals with recorded
digital/analog signals, which are discrete-time signals.
The <strong>Nyquist-Shannon sampling theorem</strong> is the fundamental
bridge between continuous-time and discrete-time signals.
It establishes a sufficient condition for a sample rate
that permits a discrete sequence of samples to capture
all the information from a continuous-time signal.
<span class="citation">(Wikipedia <a href="#ref-wiki:nyquistshannon">2020</a>)</span></p>
<p>The sample rate <span class="math inline">\(f_s\)</span> of a discrete-time signal
is defined as the number of samples per second,
its inverse is the time step between samples <span class="math inline">\(T_s\)</span>.</p>
<p>We denote, conformely to litterature a discrete signal
time frame as <span class="math inline">\(x[n]=x(t_n)\)</span> where
<span class="math inline">\(t_n=n\cdot T_s=\frac{n}{f_s}\)</span>.</p>
</div>
<div id="discrete-fourier-transform-dft" class="section level3">
<h3><span class="header-section-number">1.4.2</span> Discrete Fourier Transform (DFT)</h3>
<p>The discrete Fourier transform of <span class="math inline">\(N\)</span> samples,
with a sample rate of <span class="math inline">\(f_s\)</span> can be obtained
from its continuous definition.</p>
<p><span class="math display">\[\begin{align}
X(f) &amp;= \int\limits_{0}^{t_N} x(t)\cdot e^{-2\pi j ft}\mathrm{d}t\\
    &amp;= \lim\limits_{f_s\rightarrow\infty} \sum\limits_{n=0}^{N-1}
        x(t_n)\cdot e^{-2\pi j ft_n}\\
    &amp;= \lim\limits_{f_s\rightarrow\infty} \underbrace{\sum\limits_{n=0}^{N-1} x[n]\cdot e^{-2\pi j f \frac{n}{f_s}}}_{X[f]}\\
    &amp;= \lim\limits_{f_s\rightarrow\infty} X[f]
\end{align}\]</span></p>
<p>The DFT of <span class="math inline">\(x[n]\)</span> is given for all frequency bins
<span class="math inline">\(k=0,\ldots,K\)</span>
<span class="math display">\[X[k] = \sum\limits_{n=0}^{N-1} x[n]\cdot e^{-2\pi j k \frac{n}{f_s}}\]</span></p>
<div style="page-break-after: always;"></div>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-benetos_2013">
<p>Benetos, Emmanouil, Simon Dixon, Dimitrios Giannoulis, Holger Kirchhoff, and Anssi Klapuri. 2013. “Automatic Music Transcription: Challenges and Future Directions.” <em>Journal of Intelligent Information Systems</em> 41 (December). <a href="https://doi.org/10.1007/s10844-013-0258-3">https://doi.org/10.1007/s10844-013-0258-3</a>.</p>
</div>
<div id="ref-feynman">
<p>Feynman, Richard. 1965. “The Feynman Lectures on Physics Vol. I Ch. 47: Sound. The Wave Equation.” <a href="https://www.feynmanlectures.caltech.edu/I_47.html">https://www.feynmanlectures.caltech.edu/I_47.html</a>.</p>
</div>
<div id="ref-muller_2015">
<p>Müller, Meinard. 2015. <em>Fundamentals of Music Processing - Audio, Analysis, Algorithms, Applications</em>. Springer. <a href="https://www.audiolabs-erlangen.de/fau/professor/mueller/bookFMP">https://www.audiolabs-erlangen.de/fau/professor/mueller/bookFMP</a>.</p>
</div>
<div id="ref-wiki:nyquistshannon">
<p>Wikipedia. 2020. “Nyquist–Shannon Sampling Theorem.” <em>Wikipedia</em>. <a href="https://en.wikipedia.org/w/index.php?title=Nyquist%E2%80%93Shannon_sampling_theorem&amp;oldid=941933031">https://en.wikipedia.org/w/index.php?title=Nyquist%E2%80%93Shannon_sampling_theorem&amp;oldid=941933031</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="0_intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="2_pitch.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": {}
},
"fontsettings": false,
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["book.pdf", "PDF"]],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true,
"info": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
